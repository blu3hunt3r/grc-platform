# Part 5: Data & APIs

**Document:** 05_data_and_apis.md
**Version:** 1.0 - Complete Data Architecture
**Last Updated:** November 16, 2025
**Status:** Production-Ready Architecture
**Philosophy:** Agent-First Data Model with Real-Time Everything

---

## ğŸ“‹ **TABLE OF CONTENTS**

1. [Data Architecture Overview](#1-overview)
2. [Complete Database Schema](#2-database-schema)
3. [Agent-First Data Model](#3-agent-model)
4. [API Architecture](#4-api-architecture)
5. [Real-Time Architecture](#5-realtime)
6. [Integration Layer](#6-integrations)
7. [Caching Strategy](#7-caching)
8. [Vector Database (RAG)](#8-vector-database)
9. [Object Storage](#9-object-storage)
10. [Data Flow Examples](#10-data-flows)
11. [Migration Strategy](#11-migrations)
12. [Performance Optimization](#12-performance)

---

## **1. DATA ARCHITECTURE OVERVIEW** {#1-overview}

### **1.1 The Five Data Stores**

Our architecture uses **five specialized data stores**, each optimized for its specific purpose:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA ARCHITECTURE                            â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  POSTGRESQL (Supabase) - Primary Database           â”‚      â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚      â”‚
â”‚  â”‚  Purpose: All structured data                        â”‚      â”‚
â”‚  â”‚  Tables: 30+ tables (users, organizations, controls, â”‚      â”‚
â”‚  â”‚          evidence, approvals, agent executions, etc.)â”‚      â”‚
â”‚  â”‚  Why: ACID compliance, relationships, transactions   â”‚      â”‚
â”‚  â”‚  Access: Prisma ORM (type-safe queries)              â”‚      â”‚
â”‚  â”‚  Migration: From Neon on November 17, 2025          â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  REDIS (Upstash) - High-Speed Cache                  â”‚      â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚      â”‚
â”‚  â”‚  Purpose: Performance optimization + pub/sub         â”‚      â”‚
â”‚  â”‚  Uses: API response caching, session storage,        â”‚      â”‚
â”‚  â”‚        real-time pub/sub for live updates            â”‚      â”‚
â”‚  â”‚  Why: Sub-millisecond reads, reduces DB load 80%+    â”‚      â”‚
â”‚  â”‚  TTL Strategy: 5min-24hr depending on data type      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  PINECONE - Vector Database (RAG)                    â”‚      â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚      â”‚
â”‚  â”‚  Purpose: Semantic search for compliance knowledge   â”‚      â”‚
â”‚  â”‚  Content: SOC 2 controls, audit questions, policies, â”‚      â”‚
â”‚  â”‚          best practices, learned patterns             â”‚      â”‚
â”‚  â”‚  Why: Enables Compliance Copilot, context retrieval  â”‚      â”‚
â”‚  â”‚  Embeddings: OpenAI text-embedding-3-large (3072d)   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  SUPABASE STORAGE - Object Storage (Evidence Files) â”‚      â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚      â”‚
â”‚  â”‚  Purpose: Store binary evidence (screenshots, PDFs)  â”‚      â”‚
â”‚  â”‚  Buckets: evidence-files, policy-documents, reports  â”‚      â”‚
â”‚  â”‚  Organization: /org-id/evidence/control-id/file.pdf  â”‚      â”‚
â”‚  â”‚  Why: Integrated with DB, RLS policies, CDN delivery â”‚      â”‚
â”‚  â”‚  Access: Pre-signed URLs (security + performance)    â”‚      â”‚
â”‚  â”‚  Migration: From AWS S3 on November 17, 2025        â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  TEMPORAL - Workflow Persistence                     â”‚      â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚      â”‚
â”‚  â”‚  Purpose: Durable workflow state                     â”‚      â”‚
â”‚  â”‚  Content: Long-running audit workflows, task queues  â”‚      â”‚
â”‚  â”‚  Why: Survives crashes, provides audit trail         â”‚      â”‚
â”‚  â”‚  Access: Temporal SDK (workflow definitions)         â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **1.2 Data Flow Philosophy**

**Write Path:**
```
User Action â†’ API â†’ Agent Execution â†’ Database Write â†’ Cache Invalidation â†’ Real-Time Broadcast

Example: User approves vendor assessment
1. UI sends approval via tRPC
2. API validates user has permission
3. Database: Update approval status
4. Redis: Invalidate vendor cache
5. Supabase Realtime: Broadcast approval to all connected clients
6. Agent: Trigger next workflow step (e.g., add to registry)
```

**Read Path:**
```
User Request â†’ Check Cache â†’ Database Query â†’ Cache Update â†’ Return Data

Example: User loads dashboard
1. UI requests dashboard data via tRPC
2. API checks Redis for cached dashboard
3. If miss: Query PostgreSQL, compute metrics
4. Store results in Redis (TTL: 5 minutes)
5. Return data to UI
6. Next request within 5 min: Instant from cache
```

### **1.3 Data Consistency Model**

We use **eventual consistency** for non-critical data and **strong consistency** for critical decisions:

**Strong Consistency (Immediate):**
- User authentication (login/logout)
- Approvals (control decision-making)
- Agent execution status (prevent race conditions)
- Billing/subscription changes

**Eventual Consistency (Seconds):**
- Dashboard metrics (up to 5 min stale acceptable)
- Evidence counts (daily collection, no need for real-time)
- Activity feeds (seconds delay acceptable)
- Agent suggestions (background processing)

**Trade-off Rationale:**

Strong consistency requires database reads on every request (slow). Eventual consistency allows caching (fast). We optimize for **fast user experience** on read-heavy operations (dashboards, lists) while ensuring **correctness** on write-critical operations (approvals, billing).

---

## **2. COMPLETE DATABASE SCHEMA** {#2-database-schema}

### **2.1 Core Entity Tables**

#### **Table: organizations**

The root entity - every object belongs to an organization.

```
Purpose: Multi-tenant isolation + organizational context
Relationships: Has many users, controls, evidence, audits
Indexing: Primary key (UUID), unique on slug

Fields:
â”œâ”€ id (UUID): Unique identifier
â”œâ”€ name (String): "Acme Corp"
â”œâ”€ slug (String): "acme-corp" (used in URLs)
â”œâ”€ domain (String): "acme.com" (for email validation)
â”œâ”€ industry (Enum): SaaS | Healthcare | Finance | Other
â”œâ”€ size (Enum): 1-10 | 11-50 | 51-200 | 201-500 | 500+
â”œâ”€ subscription_tier (Enum): Free | Starter | Professional | Enterprise
â”œâ”€ subscription_status (Enum): Trial | Active | Canceled | Expired
â”œâ”€ trial_ends_at (DateTime): When trial expires
â”œâ”€ billing_email (String): "billing@acme.com"
â”œâ”€ settings (JSON): Flexible organizational preferences
â”‚   â”œâ”€ features_enabled: ["vision_evidence", "questionnaire_automation"]
â”‚   â”œâ”€ integrations: {"aws": true, "okta": true}
â”‚   â””â”€ approval_thresholds: {"vendor_assessment": 85}
â”œâ”€ created_at (DateTime)
â”œâ”€ updated_at (DateTime)
â””â”€ deleted_at (DateTime | null): Soft delete for data retention
```

**Key Decisions:**

- **Multi-tenancy approach**: Schema per tenant vs. Row-level isolation
  - Decision: Row-level isolation (all orgs share tables)
  - Why: Simpler deployment, easier cross-org analytics for us
  - Security: Prisma middleware enforces org_id filtering on all queries

- **Soft deletes**: Preserve audit trail even after org cancels
  - Retention: 90 days for free tier, 1 year for paid

---

#### **Table: users**

Individual user accounts within organizations.

```
Purpose: Authentication, authorization, activity tracking
Relationships: Belongs to organization, has many approvals, has many activities

Fields:
â”œâ”€ id (UUID)
â”œâ”€ organization_id (UUID FK â†’ organizations.id)
â”œâ”€ email (String, unique per org)
â”œâ”€ name (String): "Jane Smith"
â”œâ”€ avatar_url (String | null): Link to profile picture
â”œâ”€ role (Enum): Owner | Admin | Member | Auditor (read-only)
â”œâ”€ permissions (JSON): Fine-grained permissions
â”‚   â”œâ”€ compliance: ["view", "approve"]
â”‚   â”œâ”€ vendors: ["view", "assess", "approve"]
â”‚   â”œâ”€ agents: ["view", "configure"]
â”‚   â””â”€ settings: ["view", "edit"]
â”œâ”€ auth_user_id (String, unique): Link to Supabase Auth (migrated from Clerk on Nov 17, 2025)
â”œâ”€ last_active_at (DateTime): For session tracking
â”œâ”€ preferences (JSON): User-specific UI preferences
â”‚   â”œâ”€ dashboard_layout: "compact" | "detailed"
â”‚   â”œâ”€ notifications: {"email": true, "slack": false}
â”‚   â””â”€ timezone: "America/Los_Angeles"
â”œâ”€ created_at (DateTime)
â”œâ”€ updated_at (DateTime)
â””â”€ deleted_at (DateTime | null)
```

**Role-Based Permissions Matrix:**

| Action | Owner | Admin | Member | Auditor |
|--------|-------|-------|--------|---------|
| **View compliance data** | âœ… | âœ… | âœ… | âœ… |
| **Approve vendor assessments** | âœ… | âœ… | âŒ | âŒ |
| **Configure agents** | âœ… | âœ… | âŒ | âŒ |
| **Manage users** | âœ… | âœ… | âŒ | âŒ |
| **Billing & subscriptions** | âœ… | âŒ | âŒ | âŒ |
| **Delete organization** | âœ… | âŒ | âŒ | âŒ |

**Implementation Note**: Permission checks happen at API layer (tRPC middleware) before any database query executes.

---

#### **Table: frameworks**

Compliance frameworks (SOC 2, ISO 27001, HIPAA, etc.)

```
Purpose: Define compliance standards and their controls
Relationships: Has many controls, referenced by audits

Fields:
â”œâ”€ id (UUID)
â”œâ”€ name (String): "SOC 2 Type II"
â”œâ”€ short_name (String): "SOC2" (for display)
â”œâ”€ version (String): "2017" (frameworks evolve)
â”œâ”€ category (Enum): Security | Privacy | Healthcare | Financial
â”œâ”€ description (Text): Full framework description
â”œâ”€ control_count (Int): 150 (for progress tracking)
â”œâ”€ official_url (String): Link to AICPA/ISO documentation
â”œâ”€ is_active (Boolean): Whether framework is still used
â”œâ”€ metadata (JSON): Framework-specific data
â”‚   â”œâ”€ trust_criteria: ["Security", "Availability", "Confidentiality"]
â”‚   â”œâ”€ common_criteria_count: 64
â”‚   â””â”€ category_specific_count: 86
â”œâ”€ created_at (DateTime)
â””â”€ updated_at (DateTime)
```

**Seeded Data**: Pre-populate with:
- SOC 2 Trust Services Criteria (2017)
- ISO/IEC 27001:2022
- HIPAA Security Rule
- PCI DSS v4.0
- GDPR (data protection requirements)
- NIST Cybersecurity Framework v1.1

---

#### **Table: controls**

Individual controls within frameworks (e.g., CC6.2: Multi-factor authentication)

```
Purpose: Granular compliance requirements + evidence mapping
Relationships: Belongs to framework, has many evidence items, referenced by gaps

Fields:
â”œâ”€ id (UUID)
â”œâ”€ framework_id (UUID FK â†’ frameworks.id)
â”œâ”€ control_id (String): "CC6.2" (framework's own ID)
â”œâ”€ title (String): "Multi-Factor Authentication"
â”œâ”€ description (Text): Full control requirement
â”œâ”€ category (String): "Logical Access" (for grouping)
â”œâ”€ objective (Text): What this control aims to achieve
â”œâ”€ testing_procedures (Text[]): How auditors test this control
â”œâ”€ evidence_types (String[]): ["configuration_screenshot", "user_report", "MFA_logs"]
â”œâ”€ collection_frequency (Enum): Daily | Weekly | Monthly | Quarterly | Annual | Point-in-Time
â”œâ”€ difficulty (Enum): Low | Medium | High | Very High
â”œâ”€ common_gaps (String[]): Known issues companies face
â”œâ”€ remediation_guidance (Text): How to fix if failing
â”œâ”€ related_controls (UUID[]): Other controls that are similar
â”œâ”€ ai_prompt_template (Text): How agents should approach this control
â”œâ”€ created_at (DateTime)
â””â”€ updated_at (DateTime)
```

**AI Prompt Template Example** (for CC6.2):
```
You are assessing multi-factor authentication (MFA) implementation.

Check:
1. Is MFA enabled for all user accounts (not just admins)?
2. What MFA methods are allowed (SMS, TOTP, hardware keys)?
3. Are there any bypass mechanisms or exceptions?
4. Is MFA enforcement configured at the IDP level or per-app?

Evidence to collect:
- Screenshot of MFA policy configuration
- User enrollment report (% of users with MFA enabled)
- Sample authentication logs showing MFA challenges

Red flags:
- MFA is "optional" rather than "required"
- SMS-only MFA (vulnerable to SIM swap)
- Exceptions for certain user groups without justification
```

**Why This Matters**: When an agent assesses a control, it uses this template to know exactly what to look for and how to evaluate evidence.

---

### **2.2 Agent Execution Tables**

#### **Table: agent_executions**

Every time an agent runs, we record the execution.

```
Purpose: Audit trail + debugging + performance monitoring
Relationships: Belongs to organization, references agent, produces evidence

Fields:
â”œâ”€ id (UUID)
â”œâ”€ organization_id (UUID FK â†’ organizations.id)
â”œâ”€ agent_id (String): "access-control-agent" (from Part 4)
â”œâ”€ agent_name (String): "Access Control Agent" (display name)
â”œâ”€ task_type (Enum): Discovery | Assessment | Evidence Collection | Remediation | Questionnaire Response
â”œâ”€ status (Enum): Pending | Running | Completed | Failed | Canceled
â”œâ”€ triggered_by (Enum): User | Schedule | Event | Another Agent
â”œâ”€ trigger_context (JSON): Why this agent was invoked
â”‚   â”œâ”€ user_id: UUID (if user-triggered)
â”‚   â”œâ”€ schedule: "daily_mfa_check" (if scheduled)
â”‚   â”œâ”€ event: "new_vendor_discovered" (if event-driven)
â”‚   â””â”€ parent_execution_id: UUID (if triggered by another agent)
â”œâ”€ inputs (JSON): What the agent received as inputs
â”œâ”€ outputs (JSON): What the agent produced
â”œâ”€ reasoning (Text): Agent's chain-of-thought explanation
â”œâ”€ confidence_score (Int 0-100): How confident the agent is
â”œâ”€ tools_used (String[]): ["aws_sdk", "claude_vision", "playwright"]
â”œâ”€ llm_calls (Int): Number of LLM API calls made
â”œâ”€ total_tokens (Int): Sum of input + output tokens
â”œâ”€ cost_usd (Decimal): Estimated cost (for tracking)
â”œâ”€ started_at (DateTime)
â”œâ”€ completed_at (DateTime | null)
â”œâ”€ duration_ms (Int | null): Execution time in milliseconds
â”œâ”€ error_message (Text | null): If status = Failed
â”œâ”€ error_stack (Text | null): For debugging
â”œâ”€ retry_count (Int): How many times this execution was retried
â”œâ”€ created_at (DateTime)
â””â”€ updated_at (DateTime)
```

**Performance Metrics We Track:**

- Average execution time per agent type
- Success rate (completed / total)
- Cost per execution (for budget monitoring)
- Token usage trends (optimize prompts to reduce tokens)
- Retry patterns (identify flaky integrations)

**Querying Examples:**

```sql
-- Which agent costs the most to run?
SELECT
  agent_id,
  COUNT(*) as executions,
  AVG(cost_usd) as avg_cost,
  SUM(cost_usd) as total_cost
FROM agent_executions
WHERE organization_id = 'org-123'
  AND created_at > NOW() - INTERVAL '30 days'
GROUP BY agent_id
ORDER BY total_cost DESC;

-- Which executions are taking too long?
SELECT
  id, agent_name, task_type, duration_ms
FROM agent_executions
WHERE status = 'Running'
  AND started_at < NOW() - INTERVAL '1 hour';
```

---

#### **Table: agent_decisions**

When agents make recommendations that need human approval.

```
Purpose: Human-in-the-loop workflow + learning from feedback
Relationships: Belongs to execution, referenced by approval

Fields:
â”œâ”€ id (UUID)
â”œâ”€ organization_id (UUID FK)
â”œâ”€ execution_id (UUID FK â†’ agent_executions.id)
â”œâ”€ agent_id (String)
â”œâ”€ decision_type (Enum): Vendor Approval | Evidence Acceptance | Gap Prioritization | Policy Recommendation
â”œâ”€ recommendation (String): "APPROVE vendor XYZ as LOW RISK"
â”œâ”€ reasoning (Text): Multi-paragraph explanation
â”œâ”€ evidence_supporting (JSON[]): References to evidence
â”œâ”€ alternatives_considered (JSON[]): What else agent thought about
â”œâ”€ confidence_score (Int 0-100)
â”œâ”€ risk_level (Enum): Low | Medium | High | Critical
â”œâ”€ requires_approval (Boolean): True if human must approve
â”œâ”€ auto_approved (Boolean): True if confidence > threshold
â”œâ”€ approval_status (Enum): Pending | Approved | Rejected | Expired
â”œâ”€ approved_by_user_id (UUID FK | null)
â”œâ”€ approval_reasoning (Text | null): Why human approved/rejected
â”œâ”€ approved_at (DateTime | null)
â”œâ”€ expires_at (DateTime): Decisions expire after 7 days
â”œâ”€ created_at (DateTime)
â””â”€ updated_at (DateTime)
```

**Decision Lifecycle:**

```
1. Agent Execute â†’ Creates decision record (status: Pending)
2. If confidence > 95% AND low risk â†’ Auto-approve
3. Else â†’ Send to approval queue
4. Human reviews â†’ Approves or Rejects
5. Agent learns from feedback â†’ Adjusts future confidence thresholds
```

**Learning Loop:**

When a human approves a decision at 85% confidence, the agent learns: "Decisions at 85% confidence for this decision_type are acceptable." Next time, agent may auto-approve at 85% instead of requiring review.

When a human rejects a decision at 92% confidence, the agent learns: "I was overconfident. Increase scrutiny for this pattern."

---

### **2.3 Evidence & Compliance Tables**

#### **Table: evidence**

All evidence collected by agents (screenshots, reports, logs, etc.)

```
Purpose: Store evidence that demonstrates control effectiveness
Relationships: Belongs to organization, references control, references execution

Fields:
â”œâ”€ id (UUID)
â”œâ”€ organization_id (UUID FK)
â”œâ”€ control_id (UUID FK â†’ controls.id)
â”œâ”€ collection_method (Enum): API | Vision | Manual Upload
â”œâ”€ evidence_type (Enum): Screenshot | PDF Document | CSV Report | JSON Export | Video Recording
â”œâ”€ file_path (String): S3/R2 path to actual file
â”œâ”€ file_size_bytes (Int)
â”œâ”€ file_hash (String): SHA-256 for integrity verification
â”œâ”€ collected_at (DateTime): When evidence was gathered
â”œâ”€ collection_period_start (Date): Evidence covers from this date
â”œâ”€ collection_period_end (Date): Evidence covers until this date
â”œâ”€ agent_execution_id (UUID FK | null): Which agent collected this
â”œâ”€ manual_upload_by (UUID FK | null): If user uploaded manually
â”œâ”€ validation_status (Enum): Pending | Validated | Rejected | Expired
â”œâ”€ validated_by_agent_id (String | null): Evidence Management Agent
â”œâ”€ validated_at (DateTime | null)
â”œâ”€ validation_notes (Text | null): Why evidence was accepted/rejected
â”œâ”€ metadata (JSON): Evidence-specific data
â”‚   â”œâ”€ screenshot_url: "https://..."
â”‚   â”œâ”€ page_title: "Okta Admin Console - MFA Settings"
â”‚   â”œâ”€ timestamp_in_screenshot: "2025-11-16 14:32 UTC"
â”‚   â”œâ”€ detected_values: {"mfa_enabled": true, "users_enrolled": 247}
â”‚   â””â”€ audit_findings: [] (if auditor questioned this evidence)
â”œâ”€ tags (String[]): ["mfa", "okta", "q4_2025"]
â”œâ”€ is_public (Boolean): Can this be shown in Trust Center?
â”œâ”€ retention_until (Date): When evidence can be deleted (7 years for SOC 2)
â”œâ”€ created_at (DateTime)
â””â”€ updated_at (DateTime)
```

**Evidence Lifecycle:**

```
1. Collection â†’ Agent gathers evidence (status: Pending)
2. Validation â†’ Evidence Management Agent reviews (status: Validated/Rejected)
3. Audit â†’ Auditor reviews during audit
4. Retention â†’ Evidence kept for 7 years
5. Expiration â†’ Automated deletion after retention period
```

**Storage Calculation:**

If an organization collects 1,500 evidence items per year at average 2MB each:
- Storage: 3GB/year
- 7-year retention: 21GB total
- Cost at $0.02/GB/month: $4.20/year (very affordable)

---

#### **Table: audits**

Formal audits (SOC 2 Type II, ISO 27001, etc.)

```
Purpose: Track audit lifecycle from planning to completion
Relationships: Belongs to organization, references framework

Fields:
â”œâ”€ id (UUID)
â”œâ”€ organization_id (UUID FK)
â”œâ”€ framework_id (UUID FK â†’ frameworks.id)
â”œâ”€ audit_type (Enum): Type I | Type II | Surveillance | Certification
â”œâ”€ status (Enum): Planning | In Progress | Fieldwork | Review | Complete | Failed
â”œâ”€ audit_firm (String): "Deloitte" | "PwC" | "EY" | "KPMG"
â”œâ”€ lead_auditor_name (String): "Sarah Chen"
â”œâ”€ lead_auditor_email (String): "sarah.chen@deloitte.com"
â”œâ”€ audit_period_start (Date): 2025-01-01
â”œâ”€ audit_period_end (Date): 2025-12-31 (12 months for Type II)
â”œâ”€ fieldwork_start_date (Date): When auditors begin on-site work
â”œâ”€ fieldwork_end_date (Date): When fieldwork concludes
â”œâ”€ report_expected_date (Date): When we expect audit report
â”œâ”€ report_received_date (Date | null): When report was delivered
â”œâ”€ opinion (Enum | null): Unqualified | Qualified | Adverse | Disclaimer
â”œâ”€ findings_count (Int): Number of audit findings
â”œâ”€ observations_count (Int): Number of observations (minor issues)
â”œâ”€ evidence_package_path (String): Link to full evidence ZIP
â”œâ”€ audit_report_path (String | null): S3 path to final report PDF
â”œâ”€ cost_usd (Decimal): How much we paid for the audit
â”œâ”€ created_at (DateTime)
â”œâ”€ updated_at (DateTime)
â””â”€ completed_at (DateTime | null)
```

**Audit Timeline (SOC 2 Type II Example):**

```
Month -6: Planning phase (status: Planning)
â”œâ”€ Select audit firm
â”œâ”€ Define scope
â””â”€ Prepare evidence

Month -3 to 0: Evidence collection (status: In Progress)
â”œâ”€ Agents collect evidence daily/weekly
â”œâ”€ Evidence Management validates completeness
â””â”€ Audit Coordinator prepares package

Month 0: Fieldwork (status: Fieldwork)
â”œâ”€ Auditors review evidence package
â”œâ”€ Conduct interviews
â”œâ”€ Test controls (sample 25 items)
â””â”€ Identify findings

Month 0-1: Report drafting (status: Review)
â”œâ”€ Draft findings shared with management
â”œâ”€ Management responses prepared
â”œâ”€ Final report issued

Month 1: Completion (status: Complete)
â””â”€ Report published + opinion issued
```

---

#### **Table: audit_findings**

Issues identified by auditors during fieldwork.

```
Purpose: Track deficiencies and corrective actions
Relationships: Belongs to audit, references control

Fields:
â”œâ”€ id (UUID)
â”œâ”€ audit_id (UUID FK â†’ audits.id)
â”œâ”€ control_id (UUID FK â†’ controls.id)
â”œâ”€ finding_type (Enum): Control Deficiency | Material Weakness | Observation | Best Practice
â”œâ”€ severity (Enum): Critical | High | Medium | Low
â”œâ”€ title (String): "Emergency changes lacked post-approval"
â”œâ”€ description (Text): Full finding description from auditor
â”œâ”€ root_cause (Text): Management's root cause analysis
â”œâ”€ management_response (Text): Our response to finding
â”œâ”€ corrective_action_plan (Text): How we'll fix it
â”œâ”€ remediation_owner_id (UUID FK â†’ users.id): Who's responsible
â”œâ”€ remediation_due_date (Date)
â”œâ”€ remediation_status (Enum): Open | In Progress | Remediated | Accepted | Closed
â”œâ”€ remediation_evidence_id (UUID FK | null): Evidence of fix
â”œâ”€ auditor_validated (Boolean): Did auditor accept remediation?
â”œâ”€ created_at (DateTime)
â””â”€ updated_at (DateTime)
```

**Finding Lifecycle:**

```
1. Identified â†’ Auditor discovers issue (status: Open)
2. Response â†’ Management writes corrective action plan (status: In Progress)
3. Implementation â†’ Team fixes the issue (status: Remediated)
4. Validation â†’ Auditor verifies fix (auditor_validated: true)
5. Closure â†’ Finding closed (status: Closed)
```

---

### **2.4 Vendor & Third-Party Risk Tables**

#### **Table: vendors**

All third-party vendors/suppliers.

```
Purpose: Vendor inventory + risk assessment tracking
Relationships: Has many assessments, integrations, contracts

Fields:
â”œâ”€ id (UUID)
â”œâ”€ organization_id (UUID FK)
â”œâ”€ name (String): "Stripe"
â”œâ”€ website (String): "stripe.com"
â”œâ”€ primary_contact_name (String): "Account Manager Name"
â”œâ”€ primary_contact_email (String): "am@stripe.com"
â”œâ”€ discovery_method (Enum): Expense Report | SSO Logs | Manual Entry | API Discovery
â”œâ”€ discovered_by_agent_id (String | null): Which agent found this vendor
â”œâ”€ discovered_at (DateTime)
â”œâ”€ vendor_type (Enum): SaaS | Infrastructure | Professional Services | Hardware | Other
â”œâ”€ services_provided (Text): What does this vendor do for us?
â”œâ”€ data_processed (Enum[]): None | PII | PHI | Financial | Source Code
â”œâ”€ criticality (Enum): Critical | High | Medium | Low
â”‚   â”œâ”€ Critical: Downtime causes business outage
â”‚   â”œâ”€ High: Important but has workarounds
â”‚   â”œâ”€ Medium: Useful but not essential
â”‚   â””â”€ Low: Nice to have
â”œâ”€ annual_spend_usd (Decimal): How much we pay them/year
â”œâ”€ user_count (Int): How many of our employees use this
â”œâ”€ status (Enum): Active | Inactive | Offboarding | Offboarded
â”œâ”€ risk_score (Int 0-100): Calculated risk (higher = riskier)
â”œâ”€ last_assessment_date (Date | null)
â”œâ”€ next_assessment_due (Date | null): Based on criticality
â”œâ”€ has_soc2 (Boolean): Do they have SOC 2?
â”œâ”€ soc2_report_date (Date | null)
â”œâ”€ soc2_report_path (String | null): S3 path to their report
â”œâ”€ has_iso27001 (Boolean)
â”œâ”€ iso27001_cert_date (Date | null)
â”œâ”€ has_hipaa_baa (Boolean): Business Associate Agreement
â”œâ”€ contract_start_date (Date)
â”œâ”€ contract_end_date (Date)
â”œâ”€ auto_renewal (Boolean)
â”œâ”€ created_at (DateTime)
â””â”€ updated_at (DateTime)
```

**Vendor Criticality Assessment Logic:**

```
Critical vendors (require immediate attention):
â”œâ”€ Process customer PII/PHI/financial data
â”œâ”€ Can cause business outage if down
â”œâ”€ Annual spend > $100K
â””â”€ Used by >50% of employees

High criticality vendors:
â”œâ”€ Process sensitive data OR cause major impact
â”œâ”€ Annual spend $25K-$100K
â””â”€ Used by >25% of employees

Medium/Low: Everything else
```

**Reassessment Frequency:**

```
Critical vendors: Quarterly
High vendors: Semi-annually
Medium vendors: Annually
Low vendors: Every 2 years or on contract renewal
```

---

#### **Table: vendor_assessments**

Security assessments of vendors (questionnaires, SOC 2 reviews).

```
Purpose: Track vendor security posture over time
Relationships: Belongs to vendor

Fields:
â”œâ”€ id (UUID)
â”œâ”€ vendor_id (UUID FK â†’ vendors.id)
â”œâ”€ assessment_type (Enum): Security Questionnaire | SOC 2 Review | ISO 27001 Review | On-Site Audit | Penetration Test Review
â”œâ”€ assessment_date (Date)
â”œâ”€ assessed_by_user_id (UUID FK | null): Human reviewer
â”œâ”€ assessed_by_agent_id (String | null): "vendor-risk-agent"
â”œâ”€ risk_rating (Enum): Low | Medium | High | Critical
â”œâ”€ risk_score (Int 0-100): Calculated score
â”œâ”€ findings (JSON[]): Issues discovered
â”‚   â”œâ”€ [{
â”‚   â”‚   category: "Access Control",
â”‚   â”‚   issue: "No MFA for admin accounts",
â”‚   â”‚   severity: "High",
â”‚   â”‚   recommendation: "Require MFA for all admin access"
â”‚   â”‚  }]
â”œâ”€ strengths (String[]): Positive findings
â”œâ”€ questionnaire_responses (JSON | null): If questionnaire-based
â”œâ”€ soc2_review_notes (Text | null): If SOC 2 report reviewed
â”œâ”€ approval_status (Enum): Pending | Approved | Rejected | Conditionally Approved
â”œâ”€ approval_conditions (String[] | null): "Must enable MFA by 2026-01-01"
â”œâ”€ approved_by_user_id (UUID FK | null)
â”œâ”€ approved_at (DateTime | null)
â”œâ”€ next_assessment_due (Date): When to reassess
â”œâ”€ created_at (DateTime)
â””â”€ updated_at (DateTime)
```

**Agent-Powered Assessment Process:**

```
1. Vendor Discovery â†’ Vendor Risk Agent finds new vendor in expense report
2. Enrichment â†’ Agent searches for vendor website, SOC 2 report
3. Initial Assessment â†’ Agent analyzes public information
4. Risk Scoring â†’ Agent calculates preliminary risk score (0-100)
5. Human Review â†’ If score > threshold, send to approval queue
6. Approval â†’ Human reviews agent's assessment + approves/rejects
7. Monitoring â†’ Agent schedules next assessment based on criticality
```

---

### **2.5 Approval Workflow Tables**

#### **Table: approvals**

All items awaiting human approval (agent decisions, vendor assessments, policy changes).

```
Purpose: Central approval queue for human-in-the-loop
Relationships: Can reference multiple entity types (polymorphic)

Fields:
â”œâ”€ id (UUID)
â”œâ”€ organization_id (UUID FK)
â”œâ”€ approval_type (Enum): Vendor Assessment | Evidence Validation | Gap Remediation | Policy Change | Questionnaire Response | Agent Decision
â”œâ”€ entity_type (String): "vendor_assessment" | "agent_decision" | "policy"
â”œâ”€ entity_id (UUID): Foreign key to relevant table
â”œâ”€ title (String): "Approve Stripe as low-risk vendor"
â”œâ”€ description (Text): Context for approver
â”œâ”€ requested_by_agent_id (String): Which agent is asking
â”œâ”€ requested_by_user_id (UUID FK | null): If user-initiated
â”œâ”€ agent_recommendation (Enum): Approve | Reject | More Info Needed
â”œâ”€ agent_reasoning (Text): Why agent recommends this action
â”œâ”€ agent_confidence (Int 0-100)
â”œâ”€ evidence_links (String[]): URLs to supporting evidence
â”œâ”€ priority (Enum): Low | Medium | High | Urgent
â”œâ”€ status (Enum): Pending | Approved | Rejected | Expired
â”œâ”€ assigned_to_user_id (UUID FK | null): Specific user or role-based
â”œâ”€ reviewed_by_user_id (UUID FK | null)
â”œâ”€ review_decision (Enum | null): Approve | Reject | Request Changes
â”œâ”€ review_reasoning (Text | null): Why human decided this way
â”œâ”€ reviewed_at (DateTime | null)
â”œâ”€ expires_at (DateTime): Auto-reject if not reviewed by this time
â”œâ”€ created_at (DateTime)
â””â”€ updated_at (DateTime)
```

**Approval Queue Management:**

Users see approvals in priority order:
1. Urgent (SLA: 4 hours)
2. High (SLA: 24 hours)
3. Medium (SLA: 3 days)
4. Low (SLA: 7 days)

Expired approvals automatically reject with notification to requester.

**Learning from Approvals:**

Every approval/rejection trains the agents:

```
If user approves agent recommendation at 85% confidence:
  â†’ Agent learns: "This user trusts my decisions at 85%+"
  â†’ Next time: May auto-approve similar items at 85%

If user rejects agent recommendation at 92% confidence:
  â†’ Agent learns: "I was overconfident on this pattern"
  â†’ Next time: Lower confidence threshold, require review
```

---

## **3. AGENT-FIRST DATA MODEL** {#3-agent-model}

### **3.1 Why "Agent-First" Matters**

**Traditional GRC Data Model:**
```
User creates record â†’ System stores it â†’ Report shows it

Tables focused on:
- Assets (manually entered)
- Controls (manually mapped)
- Evidence (manually uploaded)
```

**Our Agent-First Data Model:**
```
Agent discovers data â†’ Agent creates record â†’ User reviews â†’ System stores

Tables focused on:
- Agent executions (who did what, when, why)
- Agent decisions (recommendations + reasoning)
- Human approvals (oversight + feedback)
- Learning feedback (improve over time)
```

### **3.2 Agent Execution as First-Class Entity**

Every agent action is recorded with full context:

**What We Track:**

```
WHEN did agent run?
â”œâ”€ started_at, completed_at, duration_ms

WHY did agent run?
â”œâ”€ triggered_by: User | Schedule | Event | Another Agent
â”œâ”€ trigger_context: Detailed explanation

WHAT did agent do?
â”œâ”€ inputs: What data agent received
â”œâ”€ outputs: What agent produced
â”œâ”€ tools_used: ["aws_sdk", "claude_vision"]

HOW did agent think?
â”œâ”€ reasoning: Chain-of-thought explanation
â”œâ”€ confidence_score: How certain is the agent?
â”œâ”€ alternatives_considered: What else agent thought about

DID it work?
â”œâ”€ status: Completed | Failed
â”œâ”€ error_message: If failed, why?
â”œâ”€ retry_count: How many attempts?

HOW much did it cost?
â”œâ”€ llm_calls: Number of API calls
â”œâ”€ total_tokens: Input + output tokens
â”œâ”€ cost_usd: Estimated cost
```

**Why This Granularity?**

1. **Debugging**: When agent fails, we know exactly what went wrong
2. **Performance**: Track slow executions and optimize
3. **Cost Control**: Monitor LLM usage to stay within budget
4. **Audit Trail**: Complete history for compliance
5. **Learning**: Analyze successful patterns to improve

### **3.3 Decision Provenance**

Every decision an agent makes is traceable:

```
Decision Record:
â”œâ”€ What: "Approve Stripe as low-risk vendor"
â”œâ”€ Why: "Valid SOC 2 Type II report, no breach history, used by 1000+ companies"
â”œâ”€ How Confident: 92% (high confidence)
â”œâ”€ What Evidence: [soc2_report.pdf, breach_search_results.json]
â”œâ”€ What Alternatives: "Could request on-site audit, but SOC 2 is sufficient for low-data vendors"
â”œâ”€ Human Decision: Approved
â”œâ”€ Learning: "User approved 92% confidence vendor assessment â†’ increase auto-approval threshold to 90%"
```

This creates a **decision graph** where we can trace:
- Why did we approve this vendor? (Answer: Agent ABC recommended it on DATE because REASONING)
- Why did agent recommend approval? (Answer: Found SOC 2 report + no breaches)
- Where did agent get SOC 2 report? (Answer: Execution XYZ collected it via Vision from vendor website)

**Auditor Value**: During audits, we can show complete decision provenance, not just "we approved this vendor" but WHY and WHO (agent + human) made that decision.

---

## **4. API ARCHITECTURE** {#4-api-architecture}

### **4.1 API Layers**

We use a **three-tier API architecture**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TIER 1: tRPC (Internal)                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚  Purpose: Type-safe API for Next.js frontend                    â”‚
â”‚  Authentication: Supabase Auth session-based                     â”‚
â”‚  Transport: HTTP POST to /api/trpc                               â”‚
â”‚  Validation: Zod schemas                                         â”‚
â”‚  Performance: Response caching with Redis                        â”‚
â”‚                                                                  â”‚
â”‚  Example Routes:                                                 â”‚
â”‚  â”œâ”€ dashboard.getMetrics()                                      â”‚
â”‚  â”œâ”€ approvals.list({ status: 'pending' })                      â”‚
â”‚  â”œâ”€ agents.execute({ agentId: 'discovery', params: {...} })    â”‚
â”‚  â””â”€ evidence.upload({ controlId, file })                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   TIER 2: REST API (External)                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚  Purpose: Third-party integrations + webhooks                    â”‚
â”‚  Authentication: API keys (Bearer token)                         â”‚
â”‚  Transport: RESTful HTTP (GET/POST/PUT/DELETE)                  â”‚
â”‚  Documentation: OpenAPI 3.0 spec                                â”‚
â”‚  Rate Limiting: 1000 req/hour per API key                       â”‚
â”‚                                                                  â”‚
â”‚  Example Endpoints:                                              â”‚
â”‚  â”œâ”€ GET  /api/v1/controls                                       â”‚
â”‚  â”œâ”€ POST /api/v1/evidence                                       â”‚
â”‚  â”œâ”€ POST /api/v1/webhooks/agent-completed                       â”‚
â”‚  â””â”€ GET  /api/v1/audits/{id}/report                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TIER 3: Supabase Realtime (Real-Time)               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚  Purpose: Live updates to connected clients                      â”‚
â”‚  Authentication: Supabase Auth session token in handshake        â”‚
â”‚  Transport: WebSocket over WSS (PostgreSQL replication)          â”‚
â”‚  Events: Database changes broadcast to subscribed clients        â”‚
â”‚  Benefits: RLS enforced, automatic reconnection                  â”‚
â”‚                                                                  â”‚
â”‚  Example Events:                                                 â”‚
â”‚  â”œâ”€ agent_executions INSERT/UPDATE                              â”‚
â”‚  â”œâ”€ approvals INSERT/UPDATE                                     â”‚
â”‚  â”œâ”€ evidence INSERT/UPDATE                                      â”‚
â”‚  â””â”€ Real-time progress tracking for all tables                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **4.2 tRPC Router Structure**

**Why tRPC?**

Traditional REST APIs have a fundamental problem:
```typescript
// Frontend: No type safety
const response = await fetch('/api/controls');
const data = await response.json(); // What type is this? ğŸ¤·

// Backend: Separate type definitions
app.get('/api/controls', async (req, res) => {
  // TypeScript can't connect these two worlds
});
```

**tRPC Solution:**
```typescript
// Backend defines procedure with input/output types
export const controlsRouter = router({
  list: publicProcedure
    .input(z.object({ frameworkId: z.string().uuid() }))
    .output(z.array(ControlSchema))
    .query(async ({ input, ctx }) => {
      return await ctx.db.control.findMany({
        where: { frameworkId: input.frameworkId }
      });
    })
});

// Frontend gets FULL type safety (autocomplete + errors)
const controls = await trpc.controls.list.query({ frameworkId: '...' });
// TypeScript knows controls is Control[] - autocomplete works!
```

**Router Organization:**

```
app/
â””â”€ api/
   â””â”€ trpc/
      â””â”€ routers/
         â”œâ”€ dashboard.ts      â†’ Dashboard metrics, charts
         â”œâ”€ approvals.ts      â†’ Approval queue operations
         â”œâ”€ agents.ts         â†’ Agent execution, status
         â”œâ”€ evidence.ts       â†’ Evidence upload, validation
         â”œâ”€ controls.ts       â†’ Control mappings, gaps
         â”œâ”€ frameworks.ts     â†’ Framework data
         â”œâ”€ vendors.ts        â†’ Vendor management
         â”œâ”€ audits.ts         â†’ Audit lifecycle
         â”œâ”€ users.ts          â†’ User management
         â””â”€ copilot.ts        â†’ Compliance Q&A (RAG)
```

**Example: Approval Router**

```
approvals.ts:

Procedures:
â”œâ”€ list({ status, limit, offset })
â”‚   Input: { status?: 'pending' | 'approved' | 'rejected', limit: int, offset: int }
â”‚   Output: { approvals: Approval[], total: int, hasMore: boolean }
â”‚   Auth: Requires authenticated user
â”‚   Query: SELECT * FROM approvals WHERE status = ? AND org_id = user.org_id
â”‚
â”œâ”€ getById({ id })
â”‚   Input: { id: UUID }
â”‚   Output: Approval with related entities (vendor, evidence, etc.)
â”‚   Auth: User must be in same org as approval
â”‚   Query: SELECT with JOINs to get full context
â”‚
â”œâ”€ approve({ id, reasoning })
â”‚   Input: { id: UUID, reasoning: string }
â”‚   Output: { success: boolean, approval: Approval }
â”‚   Auth: User must have approval permission
â”‚   Mutation: UPDATE approvals SET status='approved', reviewed_by=user.id
â”‚   Side Effects:
â”‚     â”œâ”€ Trigger agent to proceed with next step
â”‚     â”œâ”€ Record learning feedback (agent was right!)
â”‚     â”œâ”€ Send WebSocket event (approval.updated)
â”‚     â””â”€ Send notification to requester
â”‚
â”œâ”€ reject({ id, reasoning })
â”‚   Similar to approve but opposite
â”‚
â””â”€ bulkApprove({ ids, reasoning })
    Input: { ids: UUID[], reasoning: string }
    Output: { approved: int, failed: int }
    Use case: Approve 10 similar vendor assessments at once
```

**Caching Strategy:**

Expensive queries are cached in Redis:

```
Cacheable (5min TTL):
â”œâ”€ Dashboard metrics (recalculated every 5min)
â”œâ”€ Control list (rarely changes)
â”œâ”€ Framework data (static)

Not Cacheable (always fresh):
â”œâ”€ Approval queue (needs real-time accuracy)
â”œâ”€ Agent execution status (changes constantly)
â”œâ”€ Evidence validation (security-critical)
```

### **4.3 REST API (External Integrations)**

**Authentication:**

```
POST /api/v1/auth/api-key
{
  "name": "Production Integration",
  "scopes": ["evidence:write", "controls:read"]
}

Response:
{
  "api_key": "grc_live_kj32h4kj23h4kj2h34kjh",
  "key_id": "key_abc123",
  "scopes": ["evidence:write", "controls:read"],
  "rate_limit": 1000,
  "expires_at": null
}

Usage:
curl -H "Authorization: Bearer grc_live_kj32h4kj23h4kj2h34kjh" \
     https://api.grc.com/v1/controls
```

**Key Endpoints:**

```
Controls:
â”œâ”€ GET    /v1/controls           â†’ List all controls
â”œâ”€ GET    /v1/controls/{id}      â†’ Get control details
â”œâ”€ GET    /v1/controls/{id}/evidence â†’ Get evidence for control

Evidence:
â”œâ”€ POST   /v1/evidence           â†’ Upload evidence
â”œâ”€ GET    /v1/evidence/{id}      â†’ Get evidence details
â”œâ”€ DELETE /v1/evidence/{id}      â†’ Delete evidence

Audits:
â”œâ”€ GET    /v1/audits             â†’ List audits
â”œâ”€ GET    /v1/audits/{id}/report â†’ Download audit report PDF
â”œâ”€ POST   /v1/audits/{id}/findings â†’ Submit audit finding

Webhooks:
â”œâ”€ POST   /v1/webhooks/register  â†’ Register webhook endpoint
â”œâ”€ DELETE /v1/webhooks/{id}      â†’ Unregister webhook
â””â”€ POST   /v1/webhooks/test      â†’ Test webhook delivery
```

**Webhook Events:**

Third parties can subscribe to events:

```
Available Events:
â”œâ”€ agent.execution.completed
â”œâ”€ evidence.collected
â”œâ”€ approval.required
â”œâ”€ audit.started
â””â”€ audit.completed

Webhook Payload Example:
{
  "event": "evidence.collected",
  "timestamp": "2025-11-16T14:32:47Z",
  "organization_id": "org-abc123",
  "data": {
    "evidence_id": "ev-xyz789",
    "control_id": "CC6.2",
    "collection_method": "vision",
    "file_url": "https://evidence.grc.com/signed-url"
  }
}

Webhook Delivery:
â”œâ”€ Retry policy: 3 retries with exponential backoff
â”œâ”€ Timeout: 30 seconds
â”œâ”€ Security: HMAC signature in X-GRC-Signature header
â””â”€ Status: Track delivery success/failure in dashboard
```

---

## **5. REAL-TIME ARCHITECTURE** {#5-realtime}

### **5.1 Why Real-Time Matters**

**The User Experience Problem:**

Traditional GRC tools feel slow and opaque:
```
User: "Is the evidence collection done?"
System: (no answer - refresh page manually)

User: (Refreshes)
System: "Still in progress..." (no idea how much longer)

User: (Refreshes again 5 minutes later)
System: "Complete!" (but user wasted 5 minutes checking)
```

**Our Real-Time Solution:**

```
User: "Is the evidence collection done?"
System: (WebSocket live updates)
  â”œâ”€ "Started collecting evidence (0/47 controls)"
  â”œâ”€ "Progress: 12/47 controls complete (26%)"
  â”œâ”€ "Progress: 23/47 controls complete (49%)"
  â”œâ”€ "Progress: 47/47 controls complete (100%)"
  â””â”€ "Evidence collection complete! 3 items need your review."

User: (Never refreshes, gets live updates as they happen)
```

### **5.2 WebSocket Event Architecture**

**Connection Lifecycle:**

```
1. Client connects:
   ws://api.grc.com/ws?token=<clerk_session_token>

2. Server authenticates:
   â”œâ”€ Validate Clerk token
   â”œâ”€ Extract user_id + organization_id
   â”œâ”€ Store connection in Redis: ws:connections:{org_id} â†’ [conn1, conn2, ...]
   â””â”€ Send welcome message: {"type": "connected", "user": {...}}

3. Client subscribes to channels:
   {
     "type": "subscribe",
     "channels": ["approvals", "agents", "dashboard"]
   }

4. Server sends relevant events:
   â”œâ”€ Filter events by organization_id (security)
   â”œâ”€ Filter events by subscribed channels
   â””â”€ Send JSON over WebSocket

5. Client disconnects:
   â”œâ”€ Remove connection from Redis
   â””â”€ Clean up subscriptions
```

**Event Broadcasting:**

When something changes in the database, we broadcast to all connected clients:

```
Example: User approves a vendor assessment

1. tRPC mutation executes:
   â””â”€ UPDATE approvals SET status='approved'

2. Mutation publishes event to Redis pub/sub:
   PUBLISH approvals:updated {
     "approval_id": "apr-123",
     "status": "approved",
     "entity_type": "vendor_assessment",
     "entity_id": "va-456"
   }

3. WebSocket server receives pub/sub message:
   â”œâ”€ Looks up connections for this organization
   â”œâ”€ Filters to connections subscribed to "approvals" channel
   â””â”€ Sends event to each connection

4. Connected clients receive event:
   {
     "type": "approval.updated",
     "data": {
       "approval_id": "apr-123",
       "status": "approved",
       "title": "Stripe vendor assessment",
       "approved_by": "Jane Smith",
       "timestamp": "2025-11-16T14:32:47Z"
     }
   }

5. Frontend React updates UI:
   â”œâ”€ Remove approval from pending queue
   â”œâ”€ Show success notification
   â”œâ”€ Update dashboard metrics (pending count -1)
   â””â”€ Trigger confetti animation ğŸ‰
```

**Event Types:**

```
Agent Events:
â”œâ”€ agent.execution.started
â”œâ”€ agent.execution.progress ({ step: "Scanning AWS", progress: 45 })
â”œâ”€ agent.execution.completed
â”œâ”€ agent.execution.failed
â””â”€ agent.decision.created (new item in approval queue)

Approval Events:
â”œâ”€ approval.created
â”œâ”€ approval.updated (approved/rejected)
â””â”€ approval.expired

Evidence Events:
â”œâ”€ evidence.collected
â”œâ”€ evidence.validated
â””â”€ evidence.rejected

Dashboard Events:
â”œâ”€ dashboard.metrics.updated
â”œâ”€ dashboard.alert.created
â””â”€ dashboard.progress.updated

Audit Events:
â”œâ”€ audit.started
â”œâ”€ audit.finding.created
â””â”€ audit.completed
```

### **5.3 Scalability: Redis Pub/Sub**

**The Problem:**

With 100 organizations, each with 5 users = 500 concurrent WebSocket connections. If we store connection state in-memory on a single server:

- Server crashes â†’ all 500 connections lost
- Can't scale horizontally (connections tied to one server)
- No way to broadcast from background jobs (different process)

**The Solution: Redis Pub/Sub**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        REDIS (Central Hub)                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚  Channels:                                                       â”‚
â”‚  â”œâ”€ org:abc123:approvals                                        â”‚
â”‚  â”œâ”€ org:abc123:agents                                           â”‚
â”‚  â””â”€ org:abc123:dashboard                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘ PUBLISH                    â†“ SUBSCRIBE
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚              â”‚                   â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ Next.jsâ”‚ â”‚Backgroundâ”‚  â”‚WebSocket â”‚  â”‚  WebSocket   â”‚
â”‚  API   â”‚ â”‚  Worker  â”‚  â”‚ Server 1 â”‚  â”‚  Server 2    â”‚
â”‚        â”‚ â”‚          â”‚  â”‚          â”‚  â”‚              â”‚
â”‚Publishesâ”‚ â”‚Publishes â”‚  â”‚Subscribesâ”‚  â”‚Subscribes    â”‚
â”‚ events â”‚ â”‚  events  â”‚  â”‚ & sends  â”‚  â”‚ & sends      â”‚
â”‚        â”‚ â”‚          â”‚  â”‚to clientsâ”‚  â”‚to clients    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“                  â†“
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚Client 1-250â”‚    â”‚Client 251-500â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**How It Works:**

1. **Publishing**: Any process (API, background worker, agent) publishes events to Redis channels
2. **Subscribing**: WebSocket servers subscribe to relevant Redis channels
3. **Broadcasting**: When event arrives, WebSocket server forwards to connected clients
4. **Horizontal Scaling**: Add more WebSocket servers as load increases (all share Redis)

**Channel Naming Convention:**

```
org:{organization_id}:{event_category}

Examples:
â”œâ”€ org:abc123:approvals     â†’ All approval events for org abc123
â”œâ”€ org:abc123:agents        â†’ All agent events
â”œâ”€ org:abc123:dashboard     â†’ Dashboard updates
â””â”€ org:xyz789:approvals     â†’ Different org, isolated

Security:
â”œâ”€ Clients can ONLY subscribe to their own organization's channels
â”œâ”€ WebSocket server validates organization_id from auth token
â””â”€ Redis prevents cross-org event leakage
```

### **5.4 Fallback: Server-Sent Events (SSE)**

For environments where WebSockets are blocked (corporate firewalls, old browsers):

```
GET /api/sse?token=<clerk_session_token>

Server Response (HTTP stream):
HTTP/1.1 200 OK
Content-Type: text/event-stream
Cache-Control: no-cache
Connection: keep-alive

event: agent.execution.started
data: {"execution_id": "ex-123", "agent": "discovery"}

event: agent.execution.progress
data: {"execution_id": "ex-123", "progress": 45}

event: agent.execution.completed
data: {"execution_id": "ex-123", "status": "success"}
```

Frontend detects WebSocket support and falls back to SSE automatically.

---

## **6. INTEGRATION LAYER** {#6-integrations}

### **6.1 Integration Architecture**

We integrate with 50+ external systems. Each integration follows a common pattern:

```
Integration Structure:
lib/integrations/{service}/
â”œâ”€ client.ts         â†’ API client (SDK wrapper)
â”œâ”€ types.ts          â†’ TypeScript types for this service
â”œâ”€ auth.ts           â†’ OAuth/API key management
â”œâ”€ mappers.ts        â†’ Transform their data â†’ our data model
â”œâ”€ cache.ts          â†’ Caching strategy for this integration
â””â”€ vision-fallback.ts â†’ Playwright flows if API unavailable
```

**Example: AWS Integration**

```
lib/integrations/aws/
â”œâ”€ client.ts
â”‚   Purpose: Wrapper around AWS SDK
â”‚   Handles: Credential management, region iteration, rate limiting
â”‚   Cache: 15min TTL for infrastructure lists
â”‚
â”œâ”€ services/
â”‚   â”œâ”€ ec2.ts        â†’ List EC2 instances
â”‚   â”œâ”€ s3.ts         â†’ List buckets, check encryption
â”‚   â”œâ”€ rds.ts        â†’ List databases, check encryption
â”‚   â”œâ”€ iam.ts        â†’ List users, roles, policies
â”‚   â””â”€ cloudwatch.ts â†’ Fetch logs, metrics
â”‚
â”œâ”€ mappers.ts
â”‚   Transform AWS EC2 instance â†’ our Asset model
â”‚   Transform AWS S3 bucket â†’ our Evidence model
â”‚
â””â”€ vision-fallback.ts
    If AWS API credentials invalid:
    â”œâ”€ Launch Playwright
    â”œâ”€ Navigate to AWS Console
    â”œâ”€ Screenshot EC2 dashboard
    â””â”€ Claude Vision extracts instance list
```

### **6.2 OAuth Integration Pattern**

For services requiring OAuth (Okta, Google Workspace, Slack):

```
OAuth Flow:

1. User clicks "Connect Okta"
   â”œâ”€ Frontend: Redirect to /api/oauth/okta/authorize
   â””â”€ Backend: Generate state token (CSRF protection)

2. Backend redirects to Okta OAuth page
   URL: https://okta.com/oauth/authorize
        ?client_id=our_client_id
        &redirect_uri=https://grc.com/api/oauth/okta/callback
        &state=random_token_123
        &scope=okta.users.read okta.apps.read

3. User approves on Okta
   â””â”€ Okta redirects back: /api/oauth/okta/callback?code=abc&state=random_token_123

4. Backend validates state token
   â””â”€ Exchange code for access token + refresh token

5. Store tokens securely
   Table: integration_credentials
   â”œâ”€ organization_id
   â”œâ”€ service: "okta"
   â”œâ”€ access_token: <encrypted>
   â”œâ”€ refresh_token: <encrypted>
   â”œâ”€ expires_at: timestamp
   â””â”€ scopes: ["okta.users.read", "okta.apps.read"]

6. Use tokens for API calls
   â””â”€ Auto-refresh when expired
```

**Token Encryption:**

All OAuth tokens are encrypted at rest using AES-256:

```
Encryption Process:
â”œâ”€ Master key: Stored in environment variable (rotated quarterly)
â”œâ”€ Per-token encryption: AES-256-GCM
â”œâ”€ Storage: Encrypted token in database
â””â”€ Decryption: Only happens in-memory during API calls (never logged)
```

### **6.3 Integration Health Monitoring**

We track integration health to detect issues early:

```
Table: integration_health
â”œâ”€ organization_id
â”œâ”€ service: "okta" | "aws" | "github" | ...
â”œâ”€ status: Healthy | Degraded | Down
â”œâ”€ last_successful_call: timestamp
â”œâ”€ last_failed_call: timestamp
â”œâ”€ failure_count_24h: int
â”œâ”€ error_message: string
â””â”€ next_retry_at: timestamp

Health Check Logic:
â”œâ”€ Every integration call updates health status
â”œâ”€ If 3+ failures in 10 minutes: Mark as Degraded
â”œâ”€ If 10+ failures in 1 hour: Mark as Down
â”œâ”€ If down: Send alert to user + switch to vision fallback
â””â”€ Auto-recovery: If call succeeds, mark as Healthy
```

**User Experience:**

When integration is degraded:
```
UI shows warning:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âš ï¸ AWS Integration Degraded                         â”‚
â”‚                                                      â”‚
â”‚ We're having trouble connecting to AWS. This may    â”‚
â”‚ cause slower evidence collection. We're using       â”‚
â”‚ vision-based fallback in the meantime.              â”‚
â”‚                                                      â”‚
â”‚ [Reconnect AWS] [View Details]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## **7. CACHING STRATEGY** {#7-caching}

### **7.1 Redis Caching Layers**

**Layer 1: API Response Cache (Short TTL)**

Frequently accessed, rarely changing data:

```
Cache Key Structure:
cache:api:{route}:{params_hash}

Examples:
â”œâ”€ cache:api:controls:list:abc123          (5min TTL)
â”œâ”€ cache:api:frameworks:get:soc2           (1hr TTL)
â”œâ”€ cache:api:dashboard:metrics:org-abc123  (5min TTL)

Invalidation:
â”œâ”€ Time-based: Expire after TTL
â”œâ”€ Event-based: Delete on related mutation
â”‚   Example: When control is updated â†’ delete cache:api:controls:*
```

**Layer 2: Database Query Cache (Medium TTL)**

Expensive database queries:

```
Cache Key: cache:query:{query_signature}

Example: Dashboard metrics calculation
Query:
  SELECT COUNT(*) FROM controls WHERE status = 'implemented'
  GROUP BY framework_id

Without cache: 2.3s query time
With cache: 8ms cache hit

TTL: 5 minutes (balance freshness vs performance)
```

**Layer 3: Computed Data Cache (Long TTL)**

Expensive computations that rarely change:

```
Cache Key: cache:computed:{type}:{id}

Examples:
â”œâ”€ cache:computed:risk_score:vendor-xyz     (24hr TTL)
â”‚   Computing vendor risk score requires:
â”‚   â”œâ”€ Fetching all assessments
â”‚   â”œâ”€ Analyzing SOC 2 reports
â”‚   â”œâ”€ Checking for breaches
â”‚   â””â”€ Running ML model (expensive)
â”‚
â””â”€ cache:computed:audit_readiness:org-abc   (1hr TTL)
    Computing audit readiness requires:
    â”œâ”€ Checking 150 controls
    â”œâ”€ Validating evidence for each
    â”œâ”€ Running gap analysis
    â””â”€ Expensive calculation (10s+)
```

### **7.2 Cache Invalidation Strategy**

**Time-to-Live (TTL) Based:**

Most caches use TTL for simplicity:
```
Short-lived (5 min): Dashboard metrics, approval counts
Medium-lived (1 hr): Framework data, control lists
Long-lived (24 hr): Vendor risk scores, audit reports
```

**Event-Driven Invalidation:**

Critical paths use event-driven invalidation:

```
Event: User approves vendor assessment

Invalidations:
â”œâ”€ DELETE cache:api:approvals:list:*
â”œâ”€ DELETE cache:api:vendors:get:{vendor_id}
â”œâ”€ DELETE cache:api:dashboard:metrics:*
â””â”€ DELETE cache:computed:risk_score:{vendor_id}

Why: Ensure UI shows updated data immediately
```

**Stale-While-Revalidate:**

For acceptable staleness:

```
1. Check cache â†’ If hit, return cached data
2. If cache is >50% expired â†’ trigger background refresh
3. Next request gets fresh data

Example: Framework control list
â”œâ”€ Cache TTL: 1 hour
â”œâ”€ If cache is 35 min old: Return cached data + refresh in background
â””â”€ User always gets instant response (even if slightly stale)
```

### **7.3 Cache Performance Metrics**

We track cache effectiveness:

```
Metrics:
â”œâ”€ Hit rate: hits / (hits + misses)
â”‚   Target: >80% for frequently accessed data
â”‚
â”œâ”€ Average response time:
â”‚   â”œâ”€ Cache hit: <10ms
â”‚   â””â”€ Cache miss: 50-500ms (depends on query)
â”‚
â”œâ”€ Cache size: Monitor Redis memory usage
â”‚   Alert if >80% of allocated memory
â”‚
â””â”€ Eviction rate: How often Redis evicts keys
    High eviction = need more memory or shorter TTLs
```

---

## **8. VECTOR DATABASE (RAG)** {#8-vector-database}

### **8.1 Pinecone for Compliance Knowledge**

**Purpose**: Enable Compliance Copilot to answer questions using RAG (Retrieval-Augmented Generation).

**What We Store:**

```
Vector Database Contents:
â”œâ”€ SOC 2 Control Definitions (150 controls)
â”‚   Example: "CC6.2: Multi-factor authentication requirement"
â”‚   Embedding: OpenAI text-embedding-3-large (3072 dimensions)
â”‚
â”œâ”€ Audit Questions & Answers (500+ historical Q&A pairs)
â”‚   Example: "How do we demonstrate MFA is enforced?"
â”‚   Answer: "Provide Okta MFA policy screenshot + user enrollment report"
â”‚
â”œâ”€ Best Practices (200+ guides)
â”‚   Example: "How to implement least privilege access"
â”‚   Content: Step-by-step implementation guide
â”‚
â”œâ”€ Learned Patterns (grows over time)
â”‚   Example: "Users with 'Admin' role should have MFA"
â”‚   Source: Approved by user 15 times â†’ becomes pattern
â”‚
â””â”€ Company Policies (organization-specific)
    Example: "Acme Corp Access Control Policy v2.1"
    Private: Only accessible to Acme Corp users
```

### **8.2 Embedding Strategy**

**Document Chunking:**

SOC 2 controls are long documents. We chunk them for better retrieval:

```
Original Control (CC6.2):
"The entity restricts physical and logical access to system
 resources and information assets. The entity authorizes,
 modifies, or removes access based on rules ... [2000 words]"

Chunked:
Chunk 1 (Title): "CC6.2 - Logical Access Restriction"
Chunk 2 (Objective): "Restrict logical access to prevent unauthorized use"
Chunk 3 (Implementation): "Use multi-factor authentication for all users..."
Chunk 4 (Testing): "Auditors will review MFA policy and user enrollment..."
Chunk 5 (Evidence): "Required evidence includes policy screenshots..."

Each chunk embedded separately â†’ better semantic search
```

**Metadata for Filtering:**

Each vector includes metadata for filtering:

```
Vector Metadata:
â”œâ”€ framework: "SOC2" | "ISO27001" | "HIPAA"
â”œâ”€ control_id: "CC6.2"
â”œâ”€ category: "Logical Access"
â”œâ”€ organization_id: "org-abc" (for private docs) | null (for public docs)
â”œâ”€ document_type: "control" | "best_practice" | "policy" | "qa_pair"
â””â”€ last_updated: timestamp
```

**Why Metadata Matters:**

When Compliance Copilot searches:
```
User asks: "How do we implement MFA?"

Query with metadata filtering:
â”œâ”€ Embed query: "how to implement multi-factor authentication"
â”œâ”€ Semantic search in Pinecone with filters:
â”‚   â”œâ”€ framework: ["SOC2", "ISO27001"] (only relevant frameworks)
â”‚   â”œâ”€ organization_id: "org-abc" OR null (include public + private docs)
â”‚   â””â”€ document_type: ["control", "best_practice", "policy"]
â”œâ”€ Top 5 results returned
â””â”€ Feed to Claude: "Based on these docs, answer the user's question"
```

### **8.3 RAG Query Flow**

```
1. User asks question in Copilot:
   "What evidence do I need for access control?"

2. Frontend sends to Compliance Copilot Agent

3. Agent embeds query:
   OpenAI Embedding API â†’ 3072-dim vector

4. Agent queries Pinecone:
   â”œâ”€ Find top 10 semantically similar vectors
   â”œâ”€ Filter: organization_id = user.org OR null
   â”œâ”€ Filter: framework IN user's active frameworks
   â””â”€ Returns: Relevant control definitions + best practices

5. Agent constructs Claude prompt:
   """
   You are a compliance expert. Answer the user's question based on these documents:

   [Document 1: CC6.1 - Access Control]
   [Document 2: CC6.2 - MFA Requirement]
   [Document 3: Best Practice: Implementing Access Reviews]
   ...

   User question: "What evidence do I need for access control?"

   Provide a detailed answer with specific evidence examples.
   """

6. Claude generates answer:
   "For access control (SOC 2 CC6.x), you'll need:
    1. MFA policy screenshot (CC6.2)
    2. Quarterly access review reports (CC6.1)
    3. User provisioning/deprovisioning logs (CC6.3)
    ..."

7. Return answer to user with source citations
```

**Source Citations:**

Every answer includes sources:
```
Answer:
"For access control, you need evidence of MFA enforcement..."

Sources:
â”œâ”€ SOC 2 CC6.2 - Multi-Factor Authentication [View]
â”œâ”€ Best Practice: Implementing MFA with Okta [View]
â””â”€ Acme Corp Access Control Policy v2.1 [View]
```

This builds trust - users can verify the answer's accuracy.

---

## **9. OBJECT STORAGE** {#9-object-storage}

### **9.1 S3/R2 for Evidence Files**

**Why Object Storage?**

Evidence files (screenshots, PDFs, videos) can't go in PostgreSQL:
- Database bloat (1000 screenshots = GB of data)
- Slow queries (fetching binary data over SQL connection)
- No CDN integration (can't cache images efficiently)

Object storage (S3/Cloudflare R2) is purpose-built for files:
- Cheap: $0.01-0.02/GB/month
- Fast: CDN integration for global delivery
- Scalable: Handles petabytes
- Durable: 99.999999999% durability

**Folder Structure:**

```
grc-evidence/
â”œâ”€ {organization_id}/
â”‚   â”œâ”€ evidence/
â”‚   â”‚   â”œâ”€ {control_id}/
â”‚   â”‚   â”‚   â”œâ”€ {evidence_id}-screenshot.png
â”‚   â”‚   â”‚   â”œâ”€ {evidence_id}-report.pdf
â”‚   â”‚   â”‚   â””â”€ {evidence_id}-config.json
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€ archives/
â”‚   â”‚       â””â”€ {year}/
â”‚   â”‚           â””â”€ {control_id}/
â”‚   â”‚               â””â”€ old-evidence.png
â”‚   â”‚
â”‚   â”œâ”€ audits/
â”‚   â”‚   â””â”€ {audit_id}/
â”‚   â”‚       â”œâ”€ evidence-package.zip
â”‚   â”‚       â””â”€ audit-report.pdf
â”‚   â”‚
â”‚   â””â”€ exports/
â”‚       â””â”€ compliance-export-2025-11-16.csv
â”‚
â””â”€ public/ (accessible without auth)
    â””â”€ {organization_id}/
        â””â”€ trust-center/
            â”œâ”€ logo.png
            â”œâ”€ soc2-seal.png
            â””â”€ policy-summary.pdf
```

### **9.2 Pre-Signed URLs for Security**

**Problem**: We can't make S3 buckets public (security risk)
**Solution**: Generate temporary URLs that expire

```
Upload Flow:

1. Frontend requests upload URL:
   POST /api/evidence/upload-url
   {
     "control_id": "CC6.2",
     "file_name": "okta-mfa-screenshot.png",
     "file_type": "image/png",
     "file_size": 245678
   }

2. Backend generates pre-signed URL:
   â”œâ”€ S3 key: {org_id}/evidence/CC6.2/{uuid}-okta-mfa-screenshot.png
   â”œâ”€ Expiration: 15 minutes
   â””â”€ Allowed operations: PUT only

   Response:
   {
     "upload_url": "https://s3.amazonaws.com/grc-evidence/...<signature>",
     "evidence_id": "ev-xyz789",
     "expires_at": "2025-11-16T14:47:00Z"
   }

3. Frontend uploads directly to S3:
   PUT <upload_url>
   Content-Type: image/png
   Body: <binary file data>

4. Frontend confirms upload:
   POST /api/evidence/upload-complete
   { "evidence_id": "ev-xyz789" }

5. Backend records in database:
   INSERT INTO evidence (id, file_path, control_id, ...)
```

**Download Flow:**

```
1. Frontend requests file:
   GET /api/evidence/{id}/download

2. Backend generates pre-signed download URL:
   â”œâ”€ Verify user has permission (same org)
   â”œâ”€ Generate S3 pre-signed URL (expires in 5 min)
   â””â”€ Return URL

   Response:
   {
     "download_url": "https://cloudfront.net/...<signature>",
     "expires_at": "2025-11-16T14:52:00Z"
   }

3. Frontend redirects user to URL or displays inline
```

**CDN Integration (Cloudflare R2 + CDN):**

For frequently accessed files (Trust Center assets, SOC 2 seal):
```
Static files â†’ R2 â†’ Cloudflare CDN â†’ Global edge locations
â”œâ”€ First request: Fetch from R2, cache in CDN
â”œâ”€ Subsequent requests: Serve from nearest CDN edge (< 50ms)
â””â”€ Cost: $0 bandwidth (R2 has free egress)
```

### **9.3 Evidence Retention & Lifecycle**

**Compliance Requirements:**

SOC 2 requires 7-year evidence retention. We automate this:

```
Evidence Lifecycle:

Year 0-1: Active evidence
â”œâ”€ Location: {org_id}/evidence/{control_id}/
â”œâ”€ Access: Frequent (auditors, users)
â””â”€ Storage class: S3 Standard

Year 1-3: Recent historical evidence
â”œâ”€ Location: {org_id}/evidence/archives/{year}/
â”œâ”€ Access: Occasional (dispute resolution)
â””â”€ Storage class: S3 Infrequent Access (cheaper)

Year 3-7: Long-term retention
â”œâ”€ Location: {org_id}/evidence/archives/{year}/
â”œâ”€ Access: Rare (legal holds)
â””â”€ Storage class: S3 Glacier (90% cost reduction)

Year 7+: Deletion
â””â”€ Automatic deletion (unless legal hold applied)
```

**S3 Lifecycle Policy:**

```
Rules:
â”œâ”€ Transition to Infrequent Access after 365 days
â”œâ”€ Transition to Glacier after 1095 days (3 years)
â””â”€ Delete after 2555 days (7 years)

Legal Hold:
If organization has active lawsuit or investigation:
â”œâ”€ Apply legal hold tag to relevant evidence
â”œâ”€ Prevent deletion even after retention period
â””â”€ Manual release when legal hold lifts
```

**Cost Optimization:**

```
Example organization: 1,500 evidence items/year, 2MB average

Storage costs (7-year retention):
â”œâ”€ Year 1 (Standard): 3GB Ã— $0.023/GB = $0.84/year
â”œâ”€ Years 2-3 (Infrequent): 6GB Ã— $0.0125/GB = $0.90/year
â”œâ”€ Years 4-7 (Glacier): 12GB Ã— $0.004/GB = $0.58/year
â””â”€ Total 7-year cost: ~$2.32/year (negligible)
```

---

## **10. DATA FLOW EXAMPLES** {#10-data-flows}

### **10.1 End-to-End Flow: Evidence Collection**

Let's trace a complete evidence collection workflow from user click to database storage:

```
USER ACTION: "Collect MFA Evidence"
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: User Interface (Next.js)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User clicks "Collect Evidence" button on CC6.2 (MFA) control

Frontend executes:
â”œâ”€ const result = await trpc.agents.execute.mutate({
â”‚   agentId: 'access-control-agent',
â”‚   task: 'collect_mfa_evidence',
â”‚   controlId: 'CC6.2'
â”‚  })
â””â”€ UI shows loading state: "Access Control Agent is collecting evidence..."

â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: API Layer (tRPC Mutation)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Server receives request:
1. Validate user authentication (Clerk middleware)
2. Verify user has permission to execute agents
3. Create agent execution record:

   INSERT INTO agent_executions (
     id, organization_id, agent_id, status,
     triggered_by, trigger_context
   ) VALUES (
     'ex-abc123', 'org-xyz', 'access-control-agent', 'pending',
     'user', {'user_id': '...', 'control_id': 'CC6.2'}
   )

4. Publish WebSocket event:
   â†’ "agent.execution.started" (real-time UI update)

5. Enqueue agent task in Temporal

â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Temporal Workflow Execution                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Temporal picks up task:
â”œâ”€ Starts durable workflow: EvidenceCollectionWorkflow
â”œâ”€ Workflow state persisted (survives crashes)
â””â”€ Invokes Access Control Agent activity

â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: Agent Execution (Access Control Agent)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Agent executes multi-step collection:

PHASE 1: Determine Collection Method
â”œâ”€ Check: Does organization have Okta API credentials?
â”œâ”€ Query: integration_credentials table for Okta
â”œâ”€ Result: YES â†’ Use API-based collection
â””â”€ Update: execution record with status='running'

PHASE 2: API Collection (Okta SDK)
â”œâ”€ Tool: Okta SDK with stored credentials
â”œâ”€ API Call 1: GET /api/v1/policies/mfa
â”‚   Response: {"status": "ACTIVE", "type": "MFA_REQUIRED"}
â”‚
â”œâ”€ API Call 2: GET /api/v1/users?filter=status eq "ACTIVE"
â”‚   Response: 247 active users
â”‚
â””â”€ API Call 3: GET /api/v1/users?filter=mfaEnrolled eq true
    Response: 247 users (100% enrolled)

PHASE 3: Vision-Based Verification (Screenshot)
â”œâ”€ Launch Playwright browser
â”œâ”€ Navigate to Okta admin console
â”œâ”€ Take screenshot of MFA settings page
â”œâ”€ Upload screenshot to S3:
â”‚   Path: {org_id}/evidence/CC6.2/{uuid}-okta-mfa-settings.png
â”‚
â””â”€ Claude Vision analysis:
    Prompt: "Does this screenshot show MFA is REQUIRED?"
    Response: "Yes, the policy clearly states 'MFA REQUIRED FOR ALL USERS'"
    Confidence: 98%

PHASE 4: Create Evidence Records
â”œâ”€ INSERT INTO evidence (
â”‚   id, organization_id, control_id,
â”‚   collection_method, evidence_type, file_path,
â”‚   collected_at, validation_status
â”‚  ) VALUES (
â”‚   'ev-123', 'org-xyz', 'CC6.2',
â”‚   'api', 'json_export', '{org}/evidence/CC6.2/mfa-api-export.json',
â”‚   NOW(), 'validated'
â”‚  )
â”‚
â”œâ”€ INSERT INTO evidence (
â”‚   id, organization_id, control_id,
â”‚   collection_method, evidence_type, file_path,
â”‚   collected_at, validation_status
â”‚  ) VALUES (
â”‚   'ev-456', 'org-xyz', 'CC6.2',
â”‚   'vision', 'screenshot', '{org}/evidence/CC6.2/screenshot.png',
â”‚   NOW(), 'validated'
â”‚  )
â”‚
â””â”€ UPDATE agent_executions
    SET status='completed',
        outputs='{"evidence_collected": 2, "mfa_status": "compliant"}',
        confidence_score=98,
        completed_at=NOW()
    WHERE id='ex-abc123'

â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: Evidence Validation (Evidence Management Agent)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Evidence Management Agent automatically triggered:
â”œâ”€ Validates evidence completeness
â”œâ”€ Checks evidence covers full audit period
â”œâ”€ Verifies file integrity (checksums)
â””â”€ Updates validation_status if issues found

â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 6: Real-Time Updates (WebSocket)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Broadcast events to connected clients:

Event 1: agent.execution.progress
{
  "execution_id": "ex-abc123",
  "progress": 50,
  "step": "Collecting from Okta API"
}

Event 2: agent.execution.completed
{
  "execution_id": "ex-abc123",
  "status": "completed",
  "evidence_count": 2
}

Event 3: evidence.collected
{
  "control_id": "CC6.2",
  "evidence_ids": ["ev-123", "ev-456"],
  "collection_method": "api+vision"
}

â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 7: Frontend Updates                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

React components respond to WebSocket events:
â”œâ”€ Update progress bar: 0% â†’ 50% â†’ 100%
â”œâ”€ Show completion notification: "âœ… MFA evidence collected"
â”œâ”€ Update control status: "Not Started" â†’ "Completed"
â”œâ”€ Refresh evidence list (tRPC re-fetch)
â””â”€ Update dashboard metrics (+1 control complete)

â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 8: Cache Updates                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Redis cache invalidation:
â”œâ”€ DELETE cache:api:controls:CC6.2
â”œâ”€ DELETE cache:api:evidence:list:org-xyz
â”œâ”€ DELETE cache:api:dashboard:metrics:org-xyz
â””â”€ Next request will fetch fresh data from DB

TOTAL TIME: ~15 seconds
â”œâ”€ API calls: 3 seconds
â”œâ”€ Vision collection: 10 seconds (browser + screenshot)
â”œâ”€ Database writes: 1 second
â””â”€ Cache invalidation: 1 second
```

### **10.2 Flow: Vendor Assessment Approval**

**Scenario:** Vendor Risk Agent discovers new vendor, assesses risk, requests human approval.

```
TRIGGER: Vendor Risk Agent discovers "Stripe" in expense report

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: Vendor Discovery                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Agent queries QuickBooks API â†’ Finds $50K payment to "Stripe Inc"
2. Agent searches: Do we already have this vendor?
   â”œâ”€ Query: SELECT * FROM vendors WHERE name ILIKE '%Stripe%'
   â””â”€ Result: Not found

3. Agent enriches data:
   â”œâ”€ Search web for "Stripe security"
   â”œâ”€ Find Stripe.com â†’ Extract company info
   â””â”€ Search "Stripe SOC 2 report" â†’ Find report on vendor's Trust Center

4. Create vendor record:
   INSERT INTO vendors (
     id, organization_id, name, website,
     discovery_method, vendor_type, criticality
   ) VALUES (
     'v-123', 'org-xyz', 'Stripe', 'stripe.com',
     'expense_report', 'SaaS', 'high'
   )

â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: Risk Assessment                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Agent downloads SOC 2 report (PDF from Stripe Trust Center)
2. Claude Vision analyzes report:
   â”œâ”€ Report Type: SOC 2 Type II
   â”œâ”€ Issue Date: 2024-08-15
   â”œâ”€ Opinion: Unqualified (clean)
   â”œâ”€ Findings: 0
   â””â”€ Validity: Report is current (< 1 year old)

3. Agent calculates risk score:
   Risk Factors:
   â”œâ”€ Data processed: Payment data (HIGH risk)
   â”œâ”€ Has SOC 2: YES (+50 points)
   â”œâ”€ Clean audit: YES (+30 points)
   â”œâ”€ Criticality: High (-20 points, more scrutiny)
   â””â”€ Final Score: 60/100 (Medium-Low risk)

4. Create assessment record:
   INSERT INTO vendor_assessments (
     id, vendor_id, risk_rating, risk_score,
     approval_status, assessed_by_agent_id
   ) VALUES (
     'va-456', 'v-123', 'low', 60,
     'pending', 'vendor-risk-agent'
   )

â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: Decision & Approval Request                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Agent creates decision:
   INSERT INTO agent_decisions (
     id, execution_id, agent_id, decision_type,
     recommendation, reasoning, confidence_score
   ) VALUES (
     'd-789', 'ex-abc', 'vendor-risk-agent', 'vendor_approval',
     'APPROVE Stripe as LOW RISK',
     'Valid SOC 2 Type II report (unqualified opinion, 0 findings).
      Company is well-established payment processor used by 100k+ companies.
      Data processed: Payment info (PCI DSS Level 1 certified).
      Risk: Low - strong security posture',
     85
   )

2. Check auto-approval threshold:
   â”œâ”€ Confidence: 85%
   â”œâ”€ Organization threshold: 90% (require review below 90%)
   â””â”€ Decision: SEND TO APPROVAL QUEUE

3. Create approval:
   INSERT INTO approvals (
     id, organization_id, approval_type,
     entity_type, entity_id, title, priority,
     requested_by_agent_id, agent_recommendation
   ) VALUES (
     'apr-999', 'org-xyz', 'vendor_assessment',
     'vendor_assessment', 'va-456',
     'Approve Stripe as low-risk vendor', 'medium',
     'vendor-risk-agent', 'approve'
   )

4. Publish WebSocket event:
   â†’ "approval.created" (user sees it instantly)

â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: Human Review                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User reviews approval in UI:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vendor Risk Agent recommends:                        â”‚
â”‚ âœ… APPROVE Stripe as LOW RISK                        â”‚
â”‚                                                       â”‚
â”‚ Reasoning:                                            â”‚
â”‚ â€¢ Valid SOC 2 Type II (expires 2025-08-15)           â”‚
â”‚ â€¢ Unqualified opinion, 0 findings                    â”‚
â”‚ â€¢ PCI DSS Level 1 certified                          â”‚
â”‚ â€¢ Used by 100,000+ companies                         â”‚
â”‚                                                       â”‚
â”‚ Data Processed: Payment information                   â”‚
â”‚ Annual Spend: $50,000                                 â”‚
â”‚ Confidence: 85%                                       â”‚
â”‚                                                       â”‚
â”‚ [Approve] [Reject] [View SOC 2 Report]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User clicks "Approve"

â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 5: Approval Processing                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. tRPC mutation executes:
   UPDATE approvals
   SET status='approved',
       reviewed_by_user_id='user-123',
       review_decision='approve',
       reviewed_at=NOW()
   WHERE id='apr-999'

2. Update vendor assessment:
   UPDATE vendor_assessments
   SET approval_status='approved',
       approved_by_user_id='user-123',
       approved_at=NOW()
   WHERE id='va-456'

3. Update vendor status:
   UPDATE vendors
   SET status='active',
       risk_score=60,
       last_assessment_date=NOW(),
       next_assessment_due=NOW() + INTERVAL '6 months'
   WHERE id='v-123'

4. Record learning feedback:
   "User approved vendor_assessment at 85% confidence
    â†’ Pattern: User trusts agent at this threshold
    â†’ Action: Future similar assessments may auto-approve at 85%"

5. Publish events:
   â”œâ”€ WebSocket: "approval.updated"
   â”œâ”€ WebSocket: "vendor.approved"
   â””â”€ Redis pub/sub: Invalidate vendor caches

â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 6: Follow-Up Actions                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Schedule next assessment:
   â”œâ”€ Create calendar reminder for 6 months from now
   â””â”€ Vendor Risk Agent will auto-trigger reassessment

2. Update compliance metrics:
   â”œâ”€ Vendor registry count: +1
   â”œâ”€ Vendors with current assessments: +1
   â””â”€ Overall vendor risk score: Recalculated

3. Send notification:
   â”œâ”€ Slack: "New vendor Stripe approved (low risk)"
   â””â”€ Email: Monthly vendor report includes this approval

TOTAL TIME: 45 seconds (automated) + 2 minutes (human review) = ~3 min
```

---

## **11. MIGRATION STRATEGY** {#11-migrations}

### **11.1 Database Migration Philosophy**

**Guiding Principles:**

```
1. Zero Downtime: Migrations run without taking app offline
2. Backward Compatible: Old code can run on new schema temporarily
3. Incremental: Small changes, not big-bang rewrites
4. Tested: All migrations tested in staging before production
5. Reversible: Every migration has a rollback plan
```

### **11.2 Migration Workflow**

**Development â†’ Staging â†’ Production:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Local Development                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Engineer creates migration:
â”œâ”€ prisma/schema.prisma
â”‚   Add new column: vendor_logo_url String?
â”‚
â”œâ”€ Generate migration:
â”‚   $ npx prisma migrate dev --name add-vendor-logo
â”‚
â””â”€ Migration file created:
    prisma/migrations/20251116_add_vendor_logo/migration.sql

    ALTER TABLE "vendors" ADD COLUMN "vendor_logo_url" TEXT;

â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Code Review                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Team reviews:
â”œâ”€ Is column nullable? YES â†’ safe (won't break existing rows)
â”œâ”€ Does it have default? N/A (nullable, so default is NULL)
â”œâ”€ Index needed? NO (not queried)
â””â”€ Approved: Merge to main

â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Staging Deployment                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CI/CD runs migration on staging:
â”œâ”€ Database: Neon staging instance
â”œâ”€ Command: npx prisma migrate deploy
â”œâ”€ Result: Column added to vendors table
â”œâ”€ Test: Run full test suite
â””â”€ Verify: Check staging app works with new column

â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: Production Deployment (Zero Downtime)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Deploy to production:

Phase 1: Run migration (5 seconds)
â”œâ”€ npx prisma migrate deploy
â”œâ”€ ALTER TABLE runs on production DB
â””â”€ Table now has vendor_logo_url column

Phase 2: Deploy new code (30 seconds)
â”œâ”€ Vercel builds new app version
â”œâ”€ New version references vendor_logo_url
â””â”€ Old version (still running) ignores column (safe)

Phase 3: Cutover (0 seconds downtime)
â”œâ”€ Vercel switches traffic to new version
â”œâ”€ Old version drains connections
â””â”€ 100% traffic on new version

Result: No downtime, new column available
```

### **11.3 Complex Migration: Schema Changes**

**Scenario:** Rename table `approvals` â†’ `approval_queue`

**Problem:** Breaking change - old code won't work on renamed table.

**Solution:** Multi-step migration with views:

```
STEP 1: Create new table alongside old table

Migration 001:
CREATE TABLE "approval_queue" (LIKE "approvals" INCLUDING ALL);
INSERT INTO "approval_queue" SELECT * FROM "approvals";

Result: Both tables exist with same data

â†“
STEP 2: Create database view for backward compatibility

Migration 002:
CREATE VIEW "approvals" AS SELECT * FROM "approval_queue";

Result: Old code can still SELECT from "approvals" view

â†“
STEP 3: Deploy code that uses new table name

Code change:
- Old: db.approval.findMany()
+ New: db.approvalQueue.findMany()

Result: New code uses approval_queue, old code uses approvals view

â†“
STEP 4: Wait for all old code to stop running

Wait period: 24 hours (ensure no old deploys are still active)

â†“
STEP 5: Drop view and old table

Migration 003:
DROP VIEW "approvals";
DROP TABLE "approvals"; -- Original table, now unused

Result: Clean schema with only approval_queue
```

**Timeline:** 1 week from start to finish (ensures safety)

### **11.4 Data Backfill Migrations**

**Scenario:** Add `risk_score` column to `vendors` table, need to calculate scores for 500 existing vendors.

**Naive Approach (DON'T DO THIS):**

```sql
-- BAD: This locks the table for minutes!
ALTER TABLE vendors ADD COLUMN risk_score INT;
UPDATE vendors SET risk_score = calculate_risk_score(id);
```

**Problem:** UPDATE locks rows, blocking other queries.

**Good Approach: Batch Processing**

```
Migration:
ALTER TABLE vendors ADD COLUMN risk_score INT; -- Instant, nullable

Backfill script (runs separately):
For each batch of 100 vendors:
  1. SELECT 100 vendors WHERE risk_score IS NULL
  2. Calculate risk scores in application code
  3. UPDATE vendors SET risk_score = ? WHERE id IN (...)
  4. Sleep 1 second (avoid overwhelming DB)
  5. Repeat until all vendors processed

Progress tracking:
â”œâ”€ SELECT COUNT(*) FROM vendors WHERE risk_score IS NULL
â”œâ”€ Show: "Backfilling risk scores: 350/500 remaining"
â””â”€ Takes: 5 minutes total, but app remains responsive
```

**Best Approach: Background Job**

```
Migration:
ALTER TABLE vendors ADD COLUMN risk_score INT;

Background job (Temporal workflow):
â”œâ”€ Runs continuously after migration
â”œâ”€ Processes 10 vendors/minute (gentle load)
â”œâ”€ Completes in: 50 minutes
â””â”€ App logic handles NULL risk_scores gracefully:
    if (vendor.risk_score === null) {
      // Calculate on-the-fly for now
      vendor.risk_score = await calculateRiskScore(vendor);
    }
```

---

## **12. PERFORMANCE OPTIMIZATION** {#12-performance}

### **12.1 Database Query Optimization**

**Problem: Slow Dashboard Load**

```sql
-- BEFORE: Slow query (3.2 seconds)
SELECT
  c.id,
  c.title,
  c.status,
  COUNT(e.id) as evidence_count,
  MAX(e.collected_at) as latest_evidence
FROM controls c
LEFT JOIN evidence e ON e.control_id = c.id
WHERE c.organization_id = 'org-xyz'
GROUP BY c.id
ORDER BY c.category, c.control_id;

-- Problem: Counts evidence for EVERY control, even if 0 evidence
-- Time: 3.2 seconds for 150 controls
```

**Solution: Materialized View + Caching**

```sql
-- Create materialized view (refreshed hourly)
CREATE MATERIALIZED VIEW control_evidence_summary AS
SELECT
  c.id as control_id,
  c.organization_id,
  COUNT(e.id) as evidence_count,
  MAX(e.collected_at) as latest_evidence,
  COUNT(CASE WHEN e.validation_status = 'validated' THEN 1 END) as validated_count
FROM controls c
LEFT JOIN evidence e ON e.control_id = c.id
GROUP BY c.id, c.organization_id;

CREATE INDEX ON control_evidence_summary(organization_id);

-- AFTER: Fast query (120ms)
SELECT
  c.id,
  c.title,
  c.status,
  s.evidence_count,
  s.latest_evidence,
  s.validated_count
FROM controls c
LEFT JOIN control_evidence_summary s ON s.control_id = c.id
WHERE c.organization_id = 'org-xyz'
ORDER BY c.category, c.control_id;

-- Improvement: 3.2s â†’ 120ms (96% faster)
```

**Refresh Strategy:**

```
Refresh materialized view:
â”œâ”€ Scheduled: Every hour (background job)
â”œâ”€ On-demand: After major evidence collection
â”œâ”€ Cost: Refresh takes 2 seconds (run when DB is idle)
â””â”€ Trade-off: Dashboard shows data up to 1 hour stale (acceptable)
```

### **12.2 N+1 Query Problem**

**Problem: Loading Approval Queue**

```typescript
// BEFORE: N+1 queries
const approvals = await db.approval.findMany({
  where: { organization_id: org.id, status: 'pending' }
});

for (const approval of approvals) {
  // Extra query for EACH approval (N+1 problem!)
  const vendor = await db.vendor.findUnique({
    where: { id: approval.entity_id }
  });
  approval.vendor = vendor;
}

// Total queries: 1 + N (if 50 approvals = 51 queries = slow!)
```

**Solution: JOIN or DataLoader**

```typescript
// AFTER: Single query with JOIN
const approvals = await db.approval.findMany({
  where: { organization_id: org.id, status: 'pending' },
  include: {
    vendor: true,        // Prisma automatically JOINs
    user: true,          // Multiple includes = multiple JOINs
    agentExecution: true // All in one query
  }
});

// Total queries: 1 (50x faster!)
```

### **12.3 Index Strategy**

**Indexes We Created:**

```sql
-- Frequently queried columns
CREATE INDEX idx_evidence_organization_control
ON evidence(organization_id, control_id);

CREATE INDEX idx_approvals_organization_status
ON approvals(organization_id, status);

CREATE INDEX idx_agent_executions_organization_date
ON agent_executions(organization_id, created_at DESC);

-- Composite index for complex queries
CREATE INDEX idx_evidence_search
ON evidence(organization_id, control_id, validation_status, collected_at DESC);

-- Partial index (only pending approvals)
CREATE INDEX idx_approvals_pending
ON approvals(organization_id)
WHERE status = 'pending';
```

**How We Choose Indexes:**

```
Index if:
â”œâ”€ Column is in WHERE clause frequently
â”œâ”€ Column is used for JOIN
â”œâ”€ Column is used for ORDER BY
â””â”€ Query is slow without index (<100ms target)

Don't index if:
â”œâ”€ Column has low cardinality (few unique values)
â”œâ”€ Table is small (<1000 rows)
â”œâ”€ Column is rarely queried
â””â”€ Index would be larger than the table
```

**Monitoring:**

```sql
-- Find slow queries
SELECT
  query,
  calls,
  total_time,
  mean_time
FROM pg_stat_statements
WHERE mean_time > 100 -- Queries slower than 100ms
ORDER BY total_time DESC
LIMIT 10;

-- Find missing indexes
SELECT
  schemaname,
  tablename,
  seq_scan,
  idx_scan,
  seq_scan / NULLIF(idx_scan, 0) AS ratio
FROM pg_stat_user_tables
WHERE seq_scan > 1000 -- Table scanned frequently
  AND idx_scan < 100  -- But rarely using index
ORDER BY ratio DESC;
```

### **12.4 Application-Level Caching**

**Cache Everything That's Expensive:**

```typescript
// Framework data (rarely changes)
const frameworks = await cache.getOrSet(
  'frameworks:all',
  () => db.framework.findMany(),
  { ttl: 3600 } // 1 hour
);

// Control list (changes occasionally)
const controls = await cache.getOrSet(
  `controls:${frameworkId}`,
  () => db.control.findMany({ where: { frameworkId } }),
  { ttl: 900 } // 15 minutes
);

// Dashboard metrics (expensive calculation)
const metrics = await cache.getOrSet(
  `dashboard:metrics:${orgId}`,
  async () => {
    // Complex aggregations across multiple tables
    return calculateDashboardMetrics(orgId);
  },
  { ttl: 300 } // 5 minutes
);
```

**Cache Implementation (Redis):**

```typescript
class Cache {
  async getOrSet<T>(
    key: string,
    fn: () => Promise<T>,
    options: { ttl: number }
  ): Promise<T> {
    // Try cache first
    const cached = await redis.get(key);
    if (cached) {
      return JSON.parse(cached);
    }

    // Cache miss: Execute function
    const result = await fn();

    // Store in cache
    await redis.setex(key, options.ttl, JSON.stringify(result));

    return result;
  }

  async invalidate(pattern: string): Promise<void> {
    // Delete all keys matching pattern
    const keys = await redis.keys(pattern);
    if (keys.length > 0) {
      await redis.del(...keys);
    }
  }
}
```

### **12.5 Connection Pooling**

**Problem:** Opening new database connection for every request = slow

**Solution:** Connection pool

```
Prisma Connection Pool:
â”œâ”€ Pool size: 10 connections (for small workload)
â”œâ”€ Max wait time: 10 seconds
â”œâ”€ Connection timeout: 30 seconds
â””â”€ Idle timeout: 600 seconds (10 minutes)

Performance:
â”œâ”€ New connection: ~500ms to establish
â”œâ”€ Pooled connection: <5ms to acquire
â””â”€ Improvement: 100x faster
```

**Configuration:**

```
DATABASE_URL="postgresql://user:pass@host:5432/db?connection_limit=10&pool_timeout=10"
```

**Monitoring:**

```sql
-- Check active connections
SELECT
  count(*),
  state,
  wait_event_type
FROM pg_stat_activity
WHERE datname = 'grc_production'
GROUP BY state, wait_event_type;

-- Alert if connections > 80% of limit
-- (10 connections â†’ alert at 8+)
```

---

## **SUMMARY & NEXT STEPS**

### **What We've Covered:**

âœ… **Five Data Stores**: PostgreSQL, Redis, Pinecone, S3/R2, Temporal
âœ… **Complete Database Schema**: 30+ tables with relationships
âœ… **Agent-First Model**: Executions, decisions, approvals as first-class entities
âœ… **Three-Tier APIs**: tRPC (internal), REST (external), WebSocket (real-time)
âœ… **Real-Time Architecture**: WebSocket + Redis pub/sub for live updates
âœ… **50+ Integrations**: OAuth flows, API clients, vision fallbacks
âœ… **Caching Strategy**: Multi-layer caching with Redis (80% load reduction)
âœ… **Vector Database**: Pinecone RAG for Compliance Copilot
âœ… **Object Storage**: S3/R2 with pre-signed URLs and 7-year retention
âœ… **Data Flows**: End-to-end examples of evidence collection and approvals
âœ… **Migrations**: Zero-downtime strategy with backward compatibility
âœ… **Performance**: Query optimization, indexing, connection pooling

### **Implementation Checklist:**

**Week 1: Database Setup**
- [ ] Deploy Neon PostgreSQL
- [ ] Apply Prisma schema migrations
- [ ] Seed initial data (frameworks, controls)
- [ ] Create database indexes
- [ ] Setup connection pooling

**Week 2: Caching Layer**
- [ ] Deploy Upstash Redis
- [ ] Implement cache wrapper functions
- [ ] Add cache invalidation to mutations
- [ ] Monitor cache hit rates

**Week 3: API Implementation**
- [ ] Build tRPC routers (all 10 routers)
- [ ] Add authentication middleware
- [ ] Implement rate limiting
- [ ] Write API documentation

**Week 4: Real-Time**
- [ ] Setup WebSocket server
- [ ] Implement Redis pub/sub
- [ ] Add event broadcasting
- [ ] Test with multiple clients

**Week 5: Integrations**
- [ ] OAuth setup (Okta, AWS, GitHub)
- [ ] API client wrappers
- [ ] Vision fallback flows
- [ ] Integration health monitoring

**Week 6: Vector Database**
- [ ] Deploy Pinecone index
- [ ] Load knowledge base (SOC 2 controls)
- [ ] Implement RAG queries
- [ ] Test Compliance Copilot

**Week 7: Object Storage**
- [ ] Setup S3/R2 buckets
- [ ] Implement pre-signed URLs
- [ ] Configure lifecycle policies
- [ ] Test evidence upload/download

**Week 8: Performance**
- [ ] Create materialized views
- [ ] Add missing indexes
- [ ] Optimize slow queries
- [ ] Load testing (1000 concurrent users)

---

**Status:** âœ… Part 5 Complete
**Next Document:** [Part 6: Security & Deployment](./06_security_deployment_operations.md)
**Total Pages:** ~50 pages (comprehensive data architecture)

